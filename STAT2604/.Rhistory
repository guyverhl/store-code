)
nb.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "naive_bayes",
trControl = control)
nb.fit <- train(grade ~ V1+V2,
df.train,
metric = "ROC",
method = "naive_bayes",
trControl = control)
summary(nb.fit)
View(df.train)
nb.fit <- train(grade ~ . -grade,
df.train,
metric = "ROC",
method = "naive_bayes",
trControl = control)
nb.fit <- train(grade ~ V1+V2+V3,
df.train,
metric = "ROC",
method = "naive_bayes",
trControl = control)
summary(nb.fit)
lr.fit <- train(grade ~ V1+V2+V3,
df.train,
metric = "ROC",
method = "naive_bayes",
trControl = control)
summary(lr.fit)
lr.fit <- train(grade ~ V1+V2+V3,
df.train,
metric = "ROC",
method = "glm",
trControl = control)
summary(lr.fit)
lr.fit <- train(grade ~ V1+V2,
df.train,
metric = "ROC",
method = "glm",
trControl = control)
summary(lr.fit)
View(df.train)
lr.fit <- train(grade ~ V3+V4,
df.train,
metric = "ROC",
method = "glm",
trControl = control)
summary(lr.fit)
setwd("~/Documents/HKU/STAT2604")
df <- read.delim("ionosphere.data", header = FALSE, sep = ",", dec = ",")
View(df)
colnames(df)[35] <- "grade"
scale(df[1:34])
scale(df[, 1:34])
df[1:34] <- scale(df[1:34])
View(df)
View(df)
setwd("~/Documents/HKU/STAT2604")
df <- read.table("ionosphere.data", header = FALSE, sep = ",", dec = ",")
View(df)
df <- read.table("ionosphere.data", header = TRUE, sep = ",", dec = ",")
View(df)
df <- read.table("ionosphere.data", header = FALSE, sep = ",")
View(df)
colnames(df)[35] <- "grade"
View(df)
sum(is.na(df))
set.seed(5312)
train_size <- floor(0.8 * nrow(df))
in_rows <- sample(c(1:nrow(df)), size = train_size, replace = FALSE)
df.train <- df[in_rows, ]
df.test <- df[-in_rows, ]
control <- trainControl(
method = "cv", ## cross validation
number = 10,   ## 10-fold
summaryFunction = twoClassSummary, ## NEW
classProbs = TRUE, # IMPORTANT
verboseIter = FALSE
)
set.seed(5312)
lr.fit <- train(grade ~ V3+V4,
df.train,
metric = "ROC",
method = "glm",
trControl = control)
summary(lr.fit)
lr.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "glm",
trControl = control)
summary(lr.fit)
nb.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "naive_bayes",
trControl = control)
summary(nb.fit)
knn.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "knn",
tuneLength = 10,
trControl = control)
print(knn.fit)
plot(knn.fit)
library(kernlab)
svm.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "svmRadial",
tuneLength = 10,
trControl = control)
print(svm.fit)
plot(svm.fit)
dt.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method="rpart",
trControl=control)
summary(dt.fit)
library(rpart)
library(ranger)
rf.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method="ranger",
trControl=control)
rf.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method="ranger",
trControl=control)
summary(rf.fit)
print(rf.fit)
plot(model)
plot(rf.fit)
print(dt.fit)
plot(rf.fit)
plot(dt.fit)
glmnet.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "glmnet",
tuneGrid = expand.grid(
alpha = 0:1,
lambda = 0:10/10),
trControl = control)
glmnet.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "glmnet",
tuneGrid = expand.grid(
alpha = 0:1,
lambda = 0:10/10),
trControl = control)
print(glmnet.fit)
plot(glmnet.fit)
plot(glmnet.fit)
model_list <- list(glmmet = glmnet.fit,
rf = rf.fit,
knn = knn.fit,
svm = svm.fit,
nb = nb.fit,
lr = lr.fit,
dt = dt.fit)
resamp <- resamples(model_list)
resamp
summary(resamp)
lattice::bwplot(resamp, metric = "ROC")
View(df)
df[1:34] <- scale(df[1:34])
View(df)
setwd("~/Documents/HKU/STAT2604")
df <- read.table("ionosphere.data", header = FALSE, sep = ",")
colnames(df)[35] <- "grade"
View(df)
trainIndex <- createDataPartition(df$y, p = .8, list = FALSE)
trainIndex <- createDataPartition(df, p = .8, list = FALSE)
trainIndex <- createDataPartition(df$grade, p = .8, list = FALSE)
train <- data[ trainIndex,]
test  <- data[-trainIndex,]
train <- data[ trainIndex,]
train <- df[ trainIndex,]
test  <- df[-trainIndex,]
View(train)
set.seed(5312)
trainIndex <- createDataPartition(df$grade, p = .8, list = FALSE)
df.train <- df[ trainIndex,]
df.test  <- df[-trainIndex,]
setwd("~/Documents/HKU/STAT2604")
df <- read.table("ionosphere.data", header = FALSE, sep = ",")
colnames(df)[35] <- "grade"
sum(is.na(df))
apply(apply(df,2,is.na),2,sum) ; nrow(df)
set.seed(5312)
trainIndex <- createDataPartition(df$grade, p = .8, list = FALSE)
df.train <- df[ trainIndex,]
df.test  <- df[-trainIndex,]
control <- trainControl(
method = "cv",
number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE,
verboseIter = FALSE
)
```
set.seed(5312)
lr.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "glm",
trControl = control)
summary(lr.fit)
print(lr.fit)
pred.lr <- predict(lr.fit, newdata=df.test)
pred.lr <- predict(lr.fit, df.test)
pred.lr <- predict(lr.fit, newdata=df.test)
pred.knn <- predict(knn.fit, newdata=df.test)
set.seed(5312)
knn.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "knn",
tuneLength = 10,
trControl = control)
pred.knn <- predict(knn.fit, newdata=df.test)
pred.knn
knn.roc <- roc(response = df.test$grade, predictor = as.numeric(pred.knn))
summary(pred.knn)
control <- trainControl(
method = "cv",
number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE,
verboseIter = FALSE,
savePredictions = TRUE
)
set.seed(5312)
knn.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "knn",
tuneLength = 10,
trControl = control)
print(knn.fit)
plot(knn.fit)
pred.knn <- predict(knn.fit, newdata=df.test)
summary(pred.knn)
pred.knn <- predict(knn.fit, newdata=df.test, type="prob")
summary(pred.knn)
result.roc <- roc(df.test$grade, pred.knn$versicolor) # Draw ROC curve.
pred.knn
roc <- roc(df.test$grade, as.numeric(pred.knn))
library(caret)
pred.knn <- predict(knn.fit, newdata=df.test, type="prob")
summary(pred.knn)
roc <- roc(df.test$grade, as.numeric(pred.knn))
pred.knn <- predict(knn.fit, newdata=df.test)
summary(pred.knn)
roc <- roc(df.test$grade, as.numeric(pred.knn))
control <- trainControl(
method = "cv",
number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE,
verboseIter = FALSE
)
set.seed(5312)
knn.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "knn",
tuneLength = 10,
trControl = control)
print(knn.fit)
pred.knn <- predict(knn.fit, newdata=df.test)
summary(pred.knn)
roc <- roc(df.test$grade, as.numeric(pred.knn))
print(knn.fit)
knn.fit$pred
pred.knn <- predict(knn.fit, newdata=df.test)
control <- trainControl(
method = "cv",
number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE,
verboseIter = FALSE,
savePredictions = 'final'
)
set.seed(5312)
knn.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "knn",
tuneLength = 10,
trControl = control)
print(knn.fit)
plot(knn.fit)
pred.knn <- predict(knn.fit, newdata=df.test)
summary(pred.knn)
roc <- roc(df.test$grade, as.numeric(pred.knn))
print(knn.fit)
library(pROC)
roc <- roc(df.test$grade, as.numeric(pred.knn))
roc$auc
lr.roc <- roc(df.test$grade, as.numeric(pred.knn))
roc$auc
summary(pred.knn)
plot(knn.fit)
print(knn.fit)
pred.knn <- predict(knn.fit, newdata=df.test)
summary(pred.knn)
pred.knn
(54-16)/54
roc$auc
pred.knn <- predict(knn.fit, newdata=df.test, type="prob")
summary(pred.knn)
lr.roc <- roc(df.test$grade, as.numeric(pred.knn))
roc$auc
pred.knn <- predict(knn.fit, newdata=df.test)
summary(pred.knn)
lr.roc <- roc(df.test$grade, as.numeric(pred.knn))
roc$auc
summary(roc)
roc$percent
predictTest <- data.frame(
obs = df.test$grade,                                    ## observed class labels
predict(knn.fit, newdata = df.test, type = "prob"),         ## predicted class probabilities
pred = predict(knn.fit, newdata = df.test, type = "raw")    ## predicted class labels
)
twoClassSummary(data = predictTest, lev = levels(df.test$obs))
twoClassSummary(data = predictTest, lev = df.test$obs)
twoClassSummary(data = predictTest, lev = as.factor(levels(df.test$obs)))
v
predictTest
View(predictTest)
twoClassSummary(data = predictTest, lev = levels(predictTest$obs))
## See https://topepo.github.io/caret/measuring-performance.html#measures-for-class-probabilities
predictTest <- data.frame(
obs = df.test$grade,                                    ## observed class labels
predict(knn.fit, newdata = df.test, type = "prob"),         ## predicted class probabilities
pred = predict(knn.fit, newdata = df.test, type = "raw")    ## predicted class labels
)
twoClassSummary(data = predictTest, lev = levels(predictTest$obs))
twoClassSummary(data = predictTest, lev = levels(as.factor(predictTest$obs)))
twoClassSummary(data = predictTest, lev = predictTest$obs)
levels(predictTest$obs)
predictTest$obs
predictTest$obs =  as.factor(predictTest$obs)
twoClassSummary(data = predictTest, lev = levels(predictTest$obs))
set.seed(5312)
knn.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "knn",
tuneLength = 10,
trControl = control)
print(knn.fit)
plot(knn.fit)
pred.knn <- predict(knn.fit, newdata=df.test)
summary(pred.knn)
lr.roc <- roc(df.test$grade, as.numeric(pred.knn))
roc$auc
## AUC ROC for test data
## See https://topepo.github.io/caret/measuring-performance.html#measures-for-class-probabilities
predictTest <- data.frame(
obs = df.test$grade,                                    ## observed class labels
predict(knn.fit, newdata = df.test, type = "prob"),         ## predicted class probabilities
pred = predict(knn.fit, newdata = df.test, type = "raw")    ## predicted class labels
)
predictTest$obs =  as.factor(predictTest$obs)
twoClassSummary(data = predictTest, lev = levels(predictTest$obs))
set.seed(5312)
knn.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "knn",
tuneLength = 10,
trControl = control)
print(knn.fit)
plot(knn.fit)
pred.knn <- predict(knn.fit, newdata=df.test)
summary(pred.knn)
knn.roc <- roc(df.test$grade, as.numeric(pred.knn))
knn.roc$auc
set.seed(5312)
lr.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "glm",
trControl = control)
summary(lr.fit)
pred.lr <- predict(lr.fit, newdata=df.test)
lr.roc <- roc(df.test$grade, as.numeric(lr.roc))
lr.roc$auc
knn.roc$auc
lr.roc <- roc(df.test$grade, as.numeric(pred.lr))
lr.roc$auc
set.seed(5312)
svm.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "svmRadial",
tuneLength = 10,
trControl = control)
print(svm.fit)
plot(svm.fit)
pred.svm <- predict(svm.fit, newdata=df.test)
svn.roc <- roc(df.test$grade, as.numeric(pred.svm))
svn.roc$auc
set.seed(5312)
nb.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "naive_bayes",
trControl = control)
summary(nb.fit)
pred.nb <- predict(nb.fit, newdata=df.test)
nb.roc <- roc(df.test$grade, as.numeric(pred.nb))
nb.roc$auc
set.seed(5312)
dt.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method="rpart",
trControl=control)
print(dt.fit)
plot(dt.fit)
pred.dt <- predict(dt.fit, newdata=df.test)
dt.roc <- roc(df.test$grade, as.numeric(pred.dt))
dt.roc$auc
set.seed(5312)
rf.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method="ranger",
trControl=control)
print(rf.fit)
plot(rf.fit)
pred.rf <- predict(rf.fit, newdata=df.test)
rf.roc <- roc(df.test$grade, as.numeric(pred.rf))
rf.roc$auc
set.seed(5312)
glmnet.fit <- train(grade ~ .,
df.train,
metric = "ROC",
method = "glmnet",
tuneGrid = expand.grid(
alpha = 0:1,
lambda = 0:10/10),
trControl = control)
print(glmnet.fit)
plot(glmnet.fit)
pred.glmnet <- predict(glmnet.fit, newdata=df.test)
glmnet.roc <- roc(df.test$grade, as.numeric(pred.glmnet))
glmnet.roc$auc
set.seed(5312)
model_list <- list(glmmet = glmnet.fit,
rf = rf.fit,
knn = knn.fit,
svm = svm.fit,
nb = nb.fit,
lr = lr.fit,
dt = dt.fit)
resamp <- resamples(model_list)
resamp
summary(resamp)
lattice::bwplot(resamp, metric = "ROC")
plot(lr.roc, legacy.axes = TRUE, print.auc.y = 0.95, print.auc = TRUE, print.auc.x = 0)
plot(dt.roc, col = "blue", add = TRUE, print.auc.y = 0.55, print.auc = TRUE, print.auc.x = 0)
plot(rf.roc, col = "red" , add = TRUE, print.auc.y = 0.75, print.auc = TRUE, print.auc.x = 0)
plot(svm.roc, col = "darkgreen" , add = TRUE, print.auc.y = 0.65, print.auc = TRUE, print.auc.x = 0)
plot(nb.roc, col = "darkgreen" , add = TRUE, print.auc.y = 0.45, print.auc = TRUE, print.auc.x = 0)
plot(glmnet.roc, col = "orange" , add = TRUE, print.auc.y = 0.85, print.auc = TRUE, print.auc.x = 0)
plot(knn.roc, col = "orange" , add = TRUE, print.auc.y = 0.35, print.auc = TRUE, print.auc.x = 0)
plot(lr.roc, legacy.axes = TRUE, print.auc.y = 0.95, print.auc = TRUE, print.auc.x = 0)
plot(dt.roc, col = "blue", add = TRUE, print.auc.y = 0.55, print.auc = TRUE, print.auc.x = 0)
plot(rf.roc, col = "red" , add = TRUE, print.auc.y = 0.75, print.auc = TRUE, print.auc.x = 0)
plot(svm.roc, col = "darkgreen" , add = TRUE, print.auc.y = 0.65, print.auc = TRUE, print.auc.x = 0)
plot(nb.roc, col = "purple" , add = TRUE, print.auc.y = 0.45, print.auc = TRUE, print.auc.x = 0)
plot(glmnet.roc, col = "yellow" , add = TRUE, print.auc.y = 0.85, print.auc = TRUE, print.auc.x = 0)
plot(knn.roc, col = "orange" , add = TRUE, print.auc.y = 0.35, print.auc = TRUE, print.auc.x = 0)
plot(svm.roc, col = "darkgreen" , add = TRUE, print.auc.y = 0.65, print.auc = TRUE, print.auc.x = 0)
svm.roc <- roc(df.test$grade, as.numeric(pred.svm))
svm.roc$auc
plot(lr.roc, legacy.axes = TRUE, print.auc.y = 0.95, print.auc = TRUE, print.auc.x = 0)
plot(dt.roc, col = "blue", add = TRUE, print.auc.y = 0.55, print.auc = TRUE, print.auc.x = 0)
plot(rf.roc, col = "red" , add = TRUE, print.auc.y = 0.75, print.auc = TRUE, print.auc.x = 0)
plot(svm.roc, col = "darkgreen" , add = TRUE, print.auc.y = 0.65, print.auc = TRUE, print.auc.x = 0)
plot(nb.roc, col = "purple" , add = TRUE, print.auc.y = 0.45, print.auc = TRUE, print.auc.x = 0)
plot(glmnet.roc, col = "yellow" , add = TRUE, print.auc.y = 0.85, print.auc = TRUE, print.auc.x = 0)
plot(knn.roc, col = "orange" , add = TRUE, print.auc.y = 0.35, print.auc = TRUE, print.auc.x = 0)
set.seed(5312)
model_list <- list(glmmet = glmnet.fit,
rf = rf.fit,
knn = knn.fit,
svm = svm.fit,
nb = nb.fit,
lr = lr.fit,
dt = dt.fit)
resamp <- resamples(model_list)
resamp
summary(resamp)
lattice::bwplot(resamp, metric = "ROC")
lr.roc$auc
knn.roc$auc
svm.roc$auc
nb.roc$auc
rf.roc$auc
glmnet.roc$auc
