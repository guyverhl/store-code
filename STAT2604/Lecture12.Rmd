---
title: "Lecture 12: Functional programming"
author: "Zhonghua Liu"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Defining a function 
The following function does this by including three statements: one for computing
the mean of its input, one for getting the standard deviation, and a final expression that
returns the input scaled to be centered on the mean and having one standard deviation:
```{r}
rescale <- function(x) {
m <- mean(x)
s <- sd(x)
(x - m) / s
}
```
The first two statements are just there to define some variables you can use in the
final expression. This is typical for writing short functions.
Assignments are really also expressions. They return an object, the value that is being
assigned, and they just do so quietly. This is why if you put an assignment in parentheses
you will still get the value you assign printed. The parentheses make R remove the
invisibility of the expression result so you see the actual value:
```{r}
(x <- 1:5)
```

We usually use assignments for their side effect, assigning a name to a value, so
you might not think of them as expressions. But everything you do in R is actually an
expression. That includes control structures like if statements and for loops. They return
values and are actually functions. They return the last expression evaluated in them, just
like all other functions. Even parentheses and subscripting are functions.
If you want to return a value from a function before its last expression, you can use the
return function. It might look like a keyword, but it is a function, and you need to include
the parentheses when you use it.

Return is usually used to exit a function early and isn’t used that much in most R
code. It is easier to return a value by just making it the last expression in a function rather
than explicitly using return. But you can use it to return early like this
```{r}
rescale <- function(x, only_translate) {
m <- mean(x)
translated <- x - m
if (only_translate) return(translated)
s <- sd(x)
translated / s
}
rescale(1:4, TRUE)
rescale(1:4, FALSE)
```

This function has two arguments: x and only_translate. Your functions can have
any number of parameters. When a function takes many arguments, however, it becomes
harder to remember in which order you have to put them. To get around that problem, R
allows you to provide the arguments to a function using their names. So the two function
calls above can also be written as:

```{r}
rescale(x = 1:4, only_translate = TRUE)
rescale(x = 1:4, only_translate = FALSE)
```

#### Named Parameters and Default Parameters
If you use named arguments, the order doesn’t matter, so this is also equivalent to these
function calls:
```{r}
rescale(only_translate = TRUE, x = 1:4)
rescale(only_translate = FALSE, x = 1:4)
```

You can mix positional and named arguments. The positional arguments have to
come in the same order as that used in the function definition, and the named arguments
can come in any order. All four function calls below are equivalent:
```{r}
rescale(1:4, only_translate = TRUE)
rescale(only_translate = TRUE, 1:4)
rescale(x = 1:4, TRUE)
rescale(TRUE, x = 1:4)
```

When you provide a named argument to a function, you don’t need to use the full
parameter name. Any unique prefix will do. So we could also have used the two function
calls below:
```{r}
rescale(1:4, o = TRUE)
rescale(o = TRUE, 1:4)
```

This is convenient for interactive work with R because it saves some typing, but it
is not recommend when you are writing programs. It can easily get confusing, and if the
author of the function adds a new argument to the function with the same prefix as the
one you use, it will break your code. If the function author provides a default value for that
parameter, your code will not break if you used the full argument name.

Now default parameters are provided when the function is defined. We could have
given rescale a default parameter for only_translate like this:

```{r}
rescale <- function(x, only_translate = FALSE) {
m <- mean(x)
translated <- x - m
if (only_translate) return(translated)
s <- sd(x)
translated / s
}
```
Then, if we call the function we only need to provide x if we are happy with the
default value for only_translate:
```{r}
rescale(1:4)
```
R makes heavy use of default parameters. Many commonly used functions, such as
plotting functions and model fitting functions, have a lot of arguments. These arguments let you control in great detail what the functions do, making them very flexible, and
because they have default values, you usually only have to worry about a few of them.

### The “Gobble Up Everything Else” Parameter
There is a special parameter all functions can take which is the special variable three
dots: "...". This parameter is typically used to pass parameters on to functions called
within a function. To give an example, we can use it to deal with missing values, NA, in the
rescale function. We can write (building from the shorter version):
```{r}
rescale <- function(x, ...) {
m <- mean(x, ...)
s <- sd(x, ...)
(x - m) / s
}
```

If we give this function a vector x that contains missing values, it will return NA:
```{r}
x <- c(NA, 1:3)
rescale(x)
```
It would also have done that before because that is how the functions mean and sd
work. But both of these functions take an additional parameter, na.rm, that will make
them remove all NA values before they do their computations. The rescale function can
do the same now:
```{r}
rescale(x, na.rm = TRUE)
```

The first value in the output is still NA. Rescaling an NA value can’t be anything else.
But the rest are rescaled values where that NA was ignored when computing the mean and
standard deviation.

The ... parameter allows a function to take any named parameter at all. If you
write a function without it, it will only take the specified parameters, but if you add this
parameter, it will accept any named parameter at all:
```{r,eval=FALSE}
f <- function(x) x
g <- function(x, ...) x
f(1:4, foo = "bar")
## Error in f(1:4, foo = "bar"): unused argument (foo = "bar")
g(1:4, foo = "bar")
```
If you then call another function with ... as a parameter, all of the parameters the
first function doesn’t know about will be passed on to the second function:
```{r}
f <- function(...) list(...)
g <- function(x, y, ...) f(...)
g(x = 1, y = 2, z = 3, w = 4)
```
In the example above, function f creates a list of named elements from ..., and as
you can see it gets the parameters that g doesn’t explicitly takes.
Using ... is not particularly safe. It is often very hard to figure out what it actually
does in a particular piece of code. What is passed on to other functions depends on what
the first function explicitly takes as arguments, and when you call a second function using
it, you pass on all the parameters in it. If the function doesn’t know how to deal with
them, you get an error:

```{r,eval=FALSE}
f <- function(w) w
g <- function(x, y, ...) f(...)
g(x = 1, y = 2, z = 3, w = 4)
## Error in f(...): unused argument (z = 3)
```

In the rescale function, it would have been much better to add the rm.na parameter
explicitly.
That being said, ... is frequently used in R, particularly because many functions
take very many parameters with default values, and adding these parameters to all
functions calling them would be tedious and error-prone. It is also the best way to add
parameters when specializing generic functions, which is a topic for Object Oriented Programming in R.
To explicitly get hold of the parameters passed along in ..., you can use this
invocation: eval(substitute(alist(...))):

```{r}
parameters <- function(...) eval(substitute(alist(...)))
parameters(a = 4, b = a**2)
```
The alist function creates a list of names for each parameter and values for the
expressions given:
```{r}
alist(a = 4, b = a**2)
```

You cannot use the list function for this unless you want all the expression
evaluated. If you try to use list you can get errors like this:
```{r,eval=FALSE}
list(a = 4, b = a**2)
## Error in eval(expr, envir, enclos): object 'a' not found
```

Because R uses so-called lazy evaluation for function parameters, something we
return to shortly, it will be perfectly fine to define a function with default parameters
that are expressions that can’t necessarily be evaluated at the point where the function is
defined, but that can be evaluated inside the function. Inside a function that knows the
parameters a and b, you can evaluate expressions that use these parameters, even when
they are not defined outside the function. So the parameters given to alist above can be
used as default parameters when defining a function. But you cannot create the list using
list because it will try to evaluate the expressions.

The reason you also need to substitute and evaluate is that alist will give you exactly
the parameters you provide it. If you tried to use alist on ... you would just get ... back:

```{r}
parameters <- function(...) alist(...)
parameters(a = 4, b = x**2)
```

By substituting, we translate ... into the actual parameters given, and by evaluating,
we get the list alist would give us in this context: the list of parameters and their
associated expressions.

#### Functions Don’t Have Names
The last thing to stress when we talk about defining functions is that functions do not
have names. Variables have names, and variables can refer to functions, but these are two
separate things.
In many languages, such as Java, Python, or C++, you define a function and at the
same time you give it an argument. Whenever possible, you need a special syntax to
define a function without a name.
Not so in R. In R, functions do not have names, and when you define them, you are
not giving them a name. We have given names to all the functions we have used above
by assigning them to variables right where we defined them. We didn’t have to. It is the
function(...) ... syntax that defines a function. We are defining a function whether or
not we assign it to a variable.
We can define a function and call it immediately like this:
```{r}
(function(x) x**2)(2)
```
We would never do this, of course. Anywhere we would want to define an anonymous
function and immediately call it, we could instead just put the body of the function.
Functions that we do want to reuse we have to give a name so we can get to them again.
The syntax for defining functions, however, doesn’t force you to give them names.
When you start to write higher-order functions, that is functions that take other functions
as input or return functions, this is convenient.
Such higher-order functions are an important part of functional programming.

#### Lazy Evaluation
Expressions used in a function call are not evaluated before they are passed to the
function. Most common languages have so-called pass-by-value semantics, which means
that all expressions given to parameters of a function are evaluated before the function is
called. In R, the semantic is call-by-promise, also known as lazy evaluation.

When you call a function and give it expressions as its arguments, these are not
evaluated at that point. What the function gets is not the result of evaluating them but
the actual expressions, called promises (they are promises of an evaluation to a value you
can get when you need it). Thus the term call-by-promise. These expressions are only
evaluated when they are actually needed, thus the term lazy evaluation.
This has several consequences for how functions work. First, an expression that isn’t
used in a function isn’t evaluated:

```{r,eval=F}
f <- function(a, b) a
f(2, stop("error if evaluated"))
## [1] 2
f(stop("error if evaluated"), 2)
## Error in f(stop("error if evaluated"), 2): error if evaluated
```
If you have a parameter that can only be meaningfully evaluated in certain contexts,
it is safe enough to have it as a parameter as long as you only refer to it when those
necessary conditions are met.
It is also very useful for default values of parameters. These are evaluated inside the
scope of the function, so you can write default values that depend on other parameters:

```{r}
f <- function(a, b = a) a + b
f(a = 2)
```
This does not mean that all the expressions are evaluated inside the scope of the
function, though. You can think of two
scopes: the global scope where global variables live, and the function scope that has
parameters and local variables as well.
If you call a function like this:
```{r,eval=F}
f(a = 2, b = a)
```
you will get an error if you expect b to be the same as a inside the function. If you are
lucky, and there isn’t any global variable called a, you will get a runtime error. If you are
unlucky and there is a global variable called a, that is what b will be set to. And if you
expect it to be set to 2 here, your code will just give you an incorrect answer.

Using other parameters works for default values because these are evaluated inside
the function. The expressions you give to function calls are evaluated in the scope outside
the function.
This also means that you cannot change what an expression evaluates to just by
changing a local variable:

```{r}
a <- 4
f <- function(x) {
a <- 2
x
}
f(1 + a)
```
In this example, the expression 1 + a is evaluated inside f, but the a in the
expression is the a outside of f and not the a local variable inside f.
This is of course what you want. If expressions really were evaluated inside the scope
of the function, then you would have no idea what they evaluated to if you called a function
with an expression. It would depend on any local variables the function might use.
Because expressions are evaluated in the calling scope and not the scope of the
function, you mostly won’t notice the difference between call-by-value or call-by-promise.
There are certain cases where the difference can bite you, though, if you are not careful.
As an example, we can consider this function:

```{r}
f <- function(a) function(b) a + b
```
This might look a bit odd if you are not used to it, but it is a function that returns
another function. We will see many examples of this kind of functions in later chapters.
When we call f with a parameter a, we get a function back that will add a to its argument:
```{r}
f(2)(2)
```

We can create a list of functions from this f:
```{r}
ff <- vector("list", 4)
for (i in 1:4) {
ff[[i]] <- f(i)
}
ff
```

Here, ff contains four functions and the idea is that the first of these adds 1 to its
argument, the second add 2, and so on.
If we call the functions in ff, however, weird stuff happens:

```{r}
ff[[1]](1)
```

When we get the element ff[[1]], we get the first function we created in the loop. If
we substitute into f the value we gave in the call, this is:

```{r}
function(b) i + b
```
The parameter a from f has been set to the parameter we gave it, i, but i has not
been evaluated at this point!
When we call the function, the expression is evaluated, in the global scope, but
there i now has the value 4 because that is the last value it was assigned in the loop. The
value of i was 1 when we called it to create the function, but it is 5 when the expression is
actually evaluated.
This also means that we can change the value of i before we evaluate one of the
functions, and this changes it from the value we intended when we created the function:
```{r}
i <- 1
ff[[2]](1)
```

This laziness is only in effect the first time we call the function. If we change i again
and call the function, we get the same value as the first time the function was evaluated:
```{r}
i <- 2
ff[[2]](1)
```
We can see this in effect by looping over the functions and evaluating them:
```{r}
results <- vector("numeric", 4)
for (i in 1:4) {
  results[i] <- ff[[i]](1)
}
results
```

We have already evaluated the first two functions so they are stuck at the values they
got at the time of the first evaluation. The other two get the intended functionality but
only because we are setting the variable i in the loop where we evaluate the functions. If
we had used a different loop variable, we wouldn’t have been so lucky.
This problem is caused by having a function with an unevaluated expression
that depends on variables in the outer scope. Functions that return functions are not
uncommon, though, if you want to exploit the benefits of having a functional language.
It is also a consequence of allowing variables to change values, something most
functional programming languages do not allow for such reasons. You cannot entirely
avoid it, though, by avoiding for loops. If you call functions that loop for you, you do not
necessarily have control over how they do that, and you can end up in the same situation.
The way to avoid the problem is to force an evaluation of the parameter. If you
evaluate it once, it will remember the value, and the problem is gone.
You can do that by just writing the parameter as a single statement. That will evaluate
it. It is better to use the function force though, to make it explicit that this is what you
are doing. It really just gives you the expression back, so it works exactly as if you had just
written the parameter, but the code makes clear why you are doing it.
If you do this, the problem is gone:

```{r}
f <- function(a) {
force(a)
function(b) a + b
}
ff <- vector("list", 4)
for (i in 1:4) {
ff[[i]] <- f(i)
}
ff[[1]](1)
i <- 1
ff[[2]](1)
```

Getting back to parameters that are given expressions as arguments, we can take a
look at what they actually are represented as. We can use the function parameters we
wrote above.
If we give a function an actual value, that is what the function gets:
```{r}
parameters <- function(...) eval(substitute(alist(...)))
p <- parameters(x = 2)
class(p$x)
```
The class function here tells us the type of the parameter. For the number 2, this is
"numeric".
If we give it a name, we are giving it an expression, even if the name refers to a single
value:
```{r}
a <- 2
p <- parameters(x = a)
class(p$x)
```

The type is "name". If we try to evaluate it, it will evaluate to the value of that
parameter:
```{r}
eval(p$x)
```
It knows that the variable is in the global scope, so if we change it the expression will
reflect this if we evaluate it:
```{r}
a <- 4
eval(p$x)
```

If we call the function with an expression, the type will be "call":
```{r}
p <- parameters(x = 2 * y)
class(p$x)
```
This is because all expressions are function calls. In this case, it is the function * that
is being called.
We can only evaluate it if the variables the expression refers to are in the scope of the
expression. So evaluating this expression will give an error because y is not defined:
```{r，eval=FALSE}
eval(p$x)
## Error in eval(expr, envir, enclos): object 'y' not found
```

The parameter 'y' has to be in the calling scope, just as we saw earlier. Expressions
are evaluated in the calling scope, not inside the function, so we cannot define 'y' inside
the parameters function and get an expression we can evaluate:
```{r,eval=FALSE}
parameters2 <- function(...) {
y <- 2
eval(substitute(alist(...)))
}
p2 <- parameters(x = 2 * y)
eval(p2$x)
## Error in eval(expr, envir, enclos): object 'y' not found
```
We can set the variable and then evaluate it, though:
```{r}
y <- 2
eval(p$x)
```

Alternatively, we can explicitly set the variable in an environment given to the eval
function:
```{r}
eval(p$x, list(y = 4))
```
Actually, manipulating expressions and the scope they are evaluated in is a very
powerful tool.

### Vectorized Functions
Expressions in R are vectorized. When you write an expression, it is implicitly working on
vectors of values, not single values. Even simple expressions that only involve numbers
really are vector operations. They are just vectors of length 1.
For longer vectors, expressions work component-wise. So if you have vectors x and y,
you can subtract the first from the second component just by writing x - y:

```{r}
x <- 1:5
y <- 6:10
x - y
```

If the vectors are not of the same length, the shorter vector is just repeated as many
times as is necessary. This is why you can, for example, multiply a number to a vector:
```{r}
2 * x
```
Here 2 is a vector of length 1 and x a vector of length 5, and 2 is just repeated five
times. You will get a warning if the length of the longer vector is not divisible by the length
of the shorter vector, and you generally want to avoid this. The semantic is the same,
though: R just keeps repeating the shorter vector as many times as needed:
```{r}
x <- 1:6
y <- 1:3
x - y
```
Depending on how a function is written, it can also be used in vectorized
expressions. R is happy to use the result of a function call in a vector expression as long
as this result is a vector. This is not quite the same as the function operating componentwise
on vectors. Such functions are called vectorized functions.
Most mathematical functions such as sqrt, log, cos, and sin are vectorized, and you
can use them in expressions:
```{r}
log(1:3) - sqrt(1:3)
```

Functions you write yourself will also be vectorized if their body consists only of
vectorized expressions:
```{r}
f <- function(a, b) log(a) - sqrt(b)
f(1:3, 1:3)
```

The very first function we wrote in this book, square, was also a vectorized function.
The function scale was also, although the functions it used, mean and sd, are not; they
take vector input but return a summary of the entire vector and do not operate on the
vector component-wise.
A function that uses control structures usually will not be vectorized. We can write a
comparison function that returns -1 if the first argument is smaller than the second and 1
if the second is larger than the first, and 0 otherwise, like this:

```{r}
compare <- function(x, y) {
if (x < y) {
-1
} else if (y < x) {
1
} else {
0
}
}
```
This function will work fine on single values but not on vectors. The problem is that
the if expression only looks at the first element in the logical vector x < y. (Yes, x < y is
a vector because < is a vectorized function.)
To handle if expressions, we can get around this problem by using the ifelse
function. This is a vectorized function that behaves just as an if-else expression:

```{r}
compare <- function(x, y) {
ifelse(x < y, -1, ifelse(y < x, 1, 0))
}
compare(1:6, 1:3)
```

The situation is not always so simple that we can replace if statements with ifelse.
In most cases, we can, but when we cannot we can instead use the function Vectorize.
This function takes a function that can operate on single values and translate it into a
function that can work component-wise on vectors.
As an example, we can take the compare function from before and vectorize it:

```{r}
compare <- function(x, y) {
if (x < y) {
-1
} else if (y < x) {
1
} else {
0
}
}
compare <- Vectorize(compare)
compare(1:6, 1:3)
```

By default, Vectorize will vectorize on all parameters of a function. As an example,
imagine that we want a scale function that doesn’t scale all variables in a vector by the
same vector’s mean and standard deviation but instead uses the mean and standard
deviation of another vector:
```{r}
scale_with <- function(x, y) {
(x - mean(y)) / sd(y)
}
```

This function is already vectorized on its first parameter since it just consists of a
vectorized expression. But if we use Vectorize on it, we break it:
```{r}
scale_with(1:6, 1:3)
scale_with <- Vectorize(scale_with)
scale_with(1:6, 1:3)
```

The function we create with Vectorize is vectorized for both x and y, which means
that it operates on these component-wise. When scaling, the function only sees one
component of y, not the whole vector. The result is a vector of missing values, NA, because
the standard deviation of a single value is not defined.
We can fix this by explicitly telling Vectorize which parameters should be
vectorized, in this example, only parameter x:

```{r}
scale_with <- function(x, y) {
(x - mean(y)) / sd(y)
}
scale_with <- Vectorize(scale_with, vectorize.args="x")
scale_with(1:6, 1:3)
```

Simple functions are usually already vectorized, or can easily be made vectorized
using ifelse, but for functions more complex, the Vectorize function is needed.
As an example, we can consider a tree data structure and a function for computing
the node depth of a named node: the node depth is defined as the distance from the root.
For simplicity, we consider only binary trees. We can implement trees using lists:
```{r}
make_node <- function(name, left = NULL, right = NULL)
list(name = name, left = left, right = right)
tree <- make_node("root",
make_node("C", make_node("A"),
make_node("B")),
make_node("D"))
```

To compute the node depth, we can traverse the tree recursively:
```{r}
node_depth <- function(tree, name, depth = 0) {
if (is.null(tree)) return(NA)
if (tree$name == name) return(depth)
left <- node_depth(tree$left, name, depth + 1)
if (!is.na(left)) return(left)
right <- node_depth(tree$right, name, depth + 1)
return(right)
}
```

This is not an unreasonably complex function, but it is a function that is harder to
vectorize than the scale_with function. As it is, it works well for single names:
```{r}
node_depth(tree, "D")
node_depth(tree, "A")
```

But you will get an error if you call it on a sequence of names:
```{r}
node_depth(tree, c("A", "B", "C", "D"))
```
It is not hard to imagine that a vectorized version could be useful, however, for
example, to get the depth of a sequence of names:
```{r}
node_depth <- Vectorize(node_depth, vectorize.args = "name",
USE.NAMES = FALSE)
node_depth(tree, c("A", "B", "C", "D"))
```
Here the USE.NAMES = FALSE is needed to get a simple vector out. If we had
not included it, the vector would have names based on the input variables


### Infix Operators
Infix operators in R are also functions. You can overwrite them with other functions
(but you really shouldn’t since you could mess up a lot of code), and you can also make
your own infix operators.

User-defined infix operators are functions with special names. A function with
a name that starts and ends with the percentage sign (%) will be considered an infix
operator by R. There is no special syntax for creating a function to be used as an infix
operator, except that it should take two arguments. There is a special syntax for assigning
variables, though, including variables with names starting and ending with %.

To work with special variables you need to quote them using back-ticks. You cannot
write +(2, 2), even though + is a function. R will not understand this as a function call
when it sees it written like this. But you can take + and quote it using back-ticks, `+`, and
then it is just a variable name like all other variable names:

```{r}
`+`(2, 2)
```
The same goes for all other infix operators in R and even control structures:
```{r}
`if`(2 > 3, "true", "false")
```
The R parser recognizes control structure keywords and various operators (e.g., the
arithmetic operators), and therefore these get to have a special syntax. But they are all
really just functions. Even parentheses are a special syntax for the function `(` and the
subscript operators are as well, `[` and `[[`. If you quote them using back-ticks, you get
a variable name that you can use just like any other function name.
Just like all these operators have a special syntax, variables starting and ending with %
get a special syntax. R expects them to be infix operators and will treat an expression like this:

```{r,eval=F}
exp1 %op% exp2
```
as the function call:
```{r,eval=F}
`%op%`(exp1, exp2)
```

Knowing that you can translate an operator name into a function name just by
quoting it also tells you how to define new infix operators. If you assign a function to a
variable name, you can refer to it by that name. If that name is a quoted special name, it
gets to use the special syntax for that name.

So, to define an operator %x% that does multiplication, we can write:

```{r}
`%x%` <- `*`
3 %x% 2
```

Here we used quotes twice, first to get a variable name we could assign to for %x% and
once to get the function that the multiplication operator, *, points to.
Just because all control structures and operators in R are functions that you can
overwrite doesn't mean you should do that without extreme caution. You should never
change the functions the control structures point to, and you should not change the
other operators unless you are defining operators on new types where it is relatively
safe to do so.
Defining entirely new infix operators, however, can be quite useful if they simplify
expressions you write often.
As an example, let’s do something else with the %x% operator; after all, there is no
point in having two different operators for multiplication. We can make it replicate the
left-hand side a number of times given by the right-hand side:

```{r}
`%x%` <- function(expr, num) replicate(num, expr)
3 %x% 5
cat("This is ", "very " %x% 3, "much fun")
```
We are using the replicate function to achieve this. It does the same thing. It
repeatedly evaluates an expression a fixed number of times. Using the %x% infix might give
more readable code, depending on your taste.
In the interests of honesty, I must mention, though, that we haven’t just given
replicate a new name here, switching of arguments aside. The %x% operator works
slightly differently. In %x% the expr parameter is evaluated when we call replicate. So
if we call %x% with a function that samples random numbers, we will get the same result
repeated num times; we will not sample num times:
```{r}
rnorm(1) %x% 4
```
Lazy evaluation only takes you so far.
To actually get the same behavior as replicate, we need a little more trickery:
```{r}
`%x%` <- function(expr, num) {
m <- match.call()
replicate(num, eval.parent(m$expr))
}
rnorm(1) %x% 4
```

Here the match.call function just gets us a representation of the current function
call from which we can extract the expression without evaluating it. We then use
replicate to evaluate it a number of times in the calling function’s scope.

### Replacement Functions
Another class of functions with special names is the so-called replacement functions. Data in
R are immutable. You can change what data parameters point to, but you cannot change the
actual data. Even when it looks like you are modifying data, you are, at least conceptually,
creating a copy, modifying that, and making a variable point to the new copy.
We can see this in a simple example with vectors. If we create two vectors that point
to the same initial vector, and then modify one of them, the other remains unchanged:
```{r}
x <- y <- 1:5
x
y
x[1] <- 6
x
y
```

R is smart about it. It won’t make a copy of values if it doesn’t have to. Semantically it is
best to think of any modification as creating a copy, but for performance reasons, R will only
make a copy when it is necessary, at least for built-in data like vectors. Typically, this happens
when you have two variables referring to the same data and you “modify” one of them.

We can use the address function to get the memory address of an object. This will
change when a copy is made but will remain the same when it isn’t. And we can use the
mem_change function from the pryr package to see how much memory is allocated for
an expression. Using these two functions, we can dig a little deeper into this copying
mechanism.
We can start by creating a long-ish vector and modifying it:

```{r}
library(pryr)
rm(x)
rm(y)
mem_change(x <- 1:10000000)
address(x)
mem_change(x[1] <- 6)
address(x)
```
When we assign to the first element in this vector, we see that the entire vector is being
copied. This might look odd since you just learned that R would only copy a vector if it had
to, and here we are just modifying an element in it, and no other variable refers to it.
The reason we get a copy here is that the expression we used to create the vector,
1:10000000, creates an integer vector. The value 6 we assign to the first element is a floating
point, called numeric in R. If we want an actual integer, we have to write L after the number:

```{r}
class(6)
class(6L)
```
When we assign a numeric to an integer vector, R has to convert the entire vector into
numeric, and that is why we get a copy:
```{r}
z <- 1:5
class(z)
z[1] <- 6
class(z)
```
If we assign another numeric to it, after it has been converted, we no longer get a copy:
```{r}
mem_change(x[2] <- 7)
address(x)
```
All expression evaluations modify the memory a little, up or down, but the change is
much smaller than the entire vector so we can see that the vector isn’t being copied and
the address remains the same.
If we assign x to another variable, we do not get a copy. We just have the two names
refer to the same value:
```{r}
mem_change(y <- x)
address(x)
address(y)
```
If we change x again, though, we need a copy to make the other vector point to the
original, unmodified data:

```{r}
mem_change(x[3] <- 8)
address(x)
address(y)
```
But after that copy, we can again assign to x without necessarily making additional
copies (although what actually happens does depend on the garbage management details
in the core of the R runtime system; for me, I don’t see a copy on the second assignment):
```{r}
mem_change(x[4] <- 9)
address(x)
```
When you assign to a variable in R, you are calling the assignment function, `<-`.
When you assign to an element in a vector or list, you are using the `[<-` function. But
there is a whole class of such functions you can use to make the appearance of modifying
an object, without actually doing it of course. These are called replacement functions and
have names that end in <-. An example is the `names<-` function. If you have a vector x,
you can give its elements names using this syntax:

```{r}
x <- 1:4
x
names(x) <- letters[1:4]
x
names(x)
```

There are two different functions in play here. The last expression, which gives us
x’s names, is the names function. The function we use to assign the names to x is the
`names<-` function.
Any function you define whose name ends with <- becomes a replacement function,
and the syntax for it is that which evaluates whatever is on the right-hand side of the
assignment operator and assigns the result to the variable that it takes as its argument.
So this syntax:

```{r}
names(x) <- letters[1:4]
```
is translated into:
```{r}
x <- `names<-`(x, letters[1:4])
```
No values are harmed in the evaluation of this, but the variable is now set to the new
value.
We can write our own replacement functions using this syntax. There are just two
requirements. The function name has to end with <-, so we need to quote the name when we assign to it, and the argument for the value that goes to the right-hand side of
the assignment has to be named value. The last requirement is there so replacement
functions can take more than two arguments.

The `attr<-` function is an example of this. Attributes are key-value maps that can
be associated with objects. You can get the attributes associated with an object using
the attributes function and set all attributes with the `attributes<-` function, but
you can assign individual attributes using `attr<-`. It takes three arguments: the object
to modify, a which parameter that is the name of the attribute to set, and the value
argument that goes to the right-hand side of the assignment. The which argument is
passed to the function on the left-hand side together with the object to modify:

```{r}
x <- 1:4
attributes(x)
attributes(x) <- list(foo = "bar")
attributes(x)
attr(x, "baz") <- "qux"
attributes(x)
```

We can write a replacement function to make the tree construction we had earlier in
the chapter slightly more readable. Earlier we constructed the tree like this:
```{r}
tree <- make_node("root",
make_node("C", make_node("A"),
make_node("B")),
make_node("D"))
```

But we can make functions for setting the children of an object like this:
```{r}
`left<-` <- function(node, value) {
node$left = value
node
}
`right<-` <- function(node, value) {
node$right = value
node
}
```
And then we can construct the tree like this:
```{r}
A <- make_node("A")
B <- make_node("B")
C <- make_node("C")
D <- make_node("D")
root <- make_node("root")
left(C) <- A
right(C) <- B
left(root) <- C
right(root) <- D
tree <- root
```

To see the results, we can write a function for printing a tree. To keep the function
simple, we assume that either both children are NULL or both are trees. It is simple to extend
it to deal with trees that do not satisfy that, but it just makes the function a bit longer:
```{r}
print_tree <- function(tree) {
build_string <- function(node) {
if (is.null(node$left) && is.null(node$right)) {
node$name
} else {
left <- build_string(node$left)
right <- build_string(node$right)
paste0("(", left, ",", right, ")")
}
}
build_string(tree)
}
print_tree(tree)
```
This function shows the tree in what is known as the Newick format, which doesn’t
show the names of inner nodes, but you can see the structure of the tree. The order in which we build the tree using children is important. When we set the
children for root, we refer to the variable C. If we set the children for C after we set the
children for root, we get the old version of C, not the new modified version:

```{r}
A <- make_node("A")
B <- make_node("B")
C <- make_node("C")
D <- make_node("D")
root <- make_node("root")
left(root) <- C
right(root) <- D
left(C) <- A
right(C) <- B
tree <- root
print_tree(tree)
```

Replacement functions only look like they are modifying data. They are not. They
only reassign values to variables.


## Pure Functional Programming

A pure function is a function that behaves like a mathematical function: it maps values
from one space to another, the same value always maps to the same result, and there is no
such thing as a side effect in a mathematical function.

The level to which programming languages go to ensure that functions are pure
varies, and R does precious little in this regard. Because values are immutable, you have
some guarantee about which side effects functions can have, but not much. Functions
can modify variables outside their scope, for example, they can modify global variables.
They can print or plot and alter the state of the R process this way. They can also sample
random numbers and use them for their computations, making the results of a function
nondeterministic, for all intents and purposes, so that the same value does not always
map to the same result.

Pure functions are desirable because they are easier to reason about. If a function
does not have any side effects, you can treat it as a black box that just maps between
values. If it has side effects, you will also need to know how it modifies other parts of the
program, and that means you have to understand, at least at some level, what the body
of the function is doing. If all you need to know about a function is how it maps from
input parameters to results, you can change their implementation at any point without
breaking any code that relies on the functions.

Nondeterministic functions, those whose result is not always the same on the same
input, are not necessarily hard to reason about. They are just harder to test and debug if
their results depend on random numbers.

Since pure functions are easier to reason about and to test, you will want to write
as much of your programs using pure functions. You cannot necessarily write all your
programs in pure functions. Sometimes you need randomness to implement Monte Carlo
methods or sometimes you need functions to produce output. But if you write most of
your program using pure functions and know where the impure functions are, you are
writing better and more robust programs.


#### Writing Pure Functions
There is not much work involved in guaranteeing that a function you write is pure. You
should avoid sampling random numbers and stay away from modifying variables outside
the scope of the function.

It is trivial to avoid sampling random numbers. Don’t call any function that does this,
either directly or through other functions. If a function only calls deterministic functions
and doesn’t introduce any randomness itself, then the function will be deterministic.
It is only slightly less trivial to guarantee that you are not modifying something
outside the scope of a function. You need a special function, <<-, to modify variables
outside a function’s local scope,so avoid using
that. Then all assignments will be to local variables, and you cannot modify variables
in other scopes. If you avoid this operator, then the only risk you have of modifying
functions outside of your scope is through lazy evaluation.

Strictly speaking, we would still have a pure function if we returned functions with
such an unevaluated expression that depended on local variables. Even the functions
we would be returning would be pure. They would be referring to local variables in the
first function, variables that cannot be changed without the <<- operator once the first
function returns. They just wouldn’t necessarily be the functions you intended to return.
While such a function would be pure by the strict definition, they do have the
problem that the functions we return depend on the state of local variables inside the
first function. From a programming perspective, it doesn’t much help that the functions
are pure if they are hard to reason about, and in this case, we would need to know how
changing the state in the first function affects the functionality of the others.
A solution to avoid problems with functions depending on variables outside their
own scope, that is variables that are neither arguments nor local variables, is to simply
never change what a variable points to.

Programming languages that guarantee that functions are pure enforce this.
There simply isn’t any way to modify the value of a variable. You can define constants,
but there is no such things as variables. Since R does have variables, you will have to
avoid assigning to the same variable more than once if you want to guarantee that your
functions are pure in this way.



It is very easy to accidentally reuse a variable name in a long function, even if you
never intended to change the value of a variable. Keeping your functions short and simple
alleviates this somewhat, but there is no way to guarantee that it doesn’t happen. The
only control structure that actually forces you to change variables is for loops. You simply
cannot write a for loop without having a variable to loop over.
Now for loops have a bad reputation in R, and many will tell you to avoid them
because they are slow. This is not true. They are slow compared to loops in more lowlevel
languages, but this has nothing to do with them being loops. Because R is a very
dynamic language where everything you do involves calling functions, and functions
that can be changed at any point if someone redefines a variable, R code is just generally
slow. When you call built-in functions like sum or mean, they are fast because they are
implemented in C. By using vectorized expressions and built-in functions, you do not pay
the penalty for the dynamism. If you write a loop yourself in R, then you do. You pay the
same price, however, if you use some of the other constructions that people recommend
instead of loops; constructions will be discussed later.


The real reason you should use such constructions is that they make the intent
behind your code clearer than a loop would, at least once you get familiar with functional
programming, because you avoid the looping variable that can cause problems. The
reason people often find that their loops are inefficient is that it is very easy to write loops
that modify data, forcing R to make copies. This problem doesn’t go away just because we
avoid loops, and this is something we return to later toward the end of lecture. 


#### Recursion as Loops
The way functional programming languages avoid loops is by using recursion instead.
Anything you can write as a loop you can also write using recursive function calls, and most
of this chapter will be focusing on getting used to thinking in terms of recursive functions.
Thinking of problems as recursive is not just a programming trick for avoiding loops.
It is generally a method of breaking problems into simpler subproblems that are easier to
solve. When you have to address a problem, you can first consider whether there are base
cases that are trivial to solve. If we want to search for an element in a sequence, it is trivial
to determine if it is there if the sequence is empty. Then it obviously isn’t there. Now, if
the sequence isn’t empty, we have a more difficult problem, but we can break it into two
smaller problems. Is the element we are searching for the first element in the sequence?
If so, the element is there. If the first element is not equal to the element we are searching
for, then it is only in the sequence if it is the remainder of the sequence.
We can write a linear search function based on this breakdown of the problem.
We will first check for the base case and return FALSE if we are searching in an empty
sequence. Otherwise, we check the first element, and if we find it, we return TRUE, and if it
wasn’t the first element, we call the function recursively on the rest of the sequence:

```{r}
lin_search <- function(element, sequence) {
if (is_empty(sequence)) FALSE
else if (first(sequence) == element) TRUE
else lin_search(element, rest(sequence))
}
```

We have hidden away the test for emptiness, the extraction of the first element,
and the remainder of the sequence in three functions, is_empty, first, and rest. For a
vector, they can be implemented like this:

```{r}
is_empty <- function(x) length(x) == 0
first <- function(x) x[1]
rest <- function(x) {
if (length(x) == 1) NULL else x[2:length(x)]
}
```

A vector is empty if it has length zero. The first element is of course just the first
element. The rest function is a little more involved. The indexing 2:length(x) will give
us the vector 2 1 if x has length 1, so I handle that case explicitly.

We can test the function on a few cases to assure ourselves that it works:

```{r}
x <- 1:5
lin_search(0, x)
lin_search(1, x)
lin_search(5, x)
lin_search(6, x)
```
Now this search algorithm works, but you should never write code like the rest
function in it. The way we extract the rest of a vector by slicing will make R copy that
subvector. The first time we call rest, we get the entire vector minus the first element, the
second time we get the entire vector minus the first two, and so on. This adds up to about
half the length of the vector squared. So while the search algorithm should be run in
linear time, the way we extract the rest of a vector makes it run in quadratic time.
In practice, this doesn’t matter. There is a limit in R on how deep we can go in
recursive calls, and we will reach that limit long before performance becomes an issue. We
return to these issues at the end of the chapter, but for now let’s just, for aesthetic reasons,
avoid a quadratic running-time algorithm if we can make a linear-time algorithm.

#### The Structure of a Recursive Function
Recursive functions all follow the same pattern: figure out the base cases that are easy
to solve, and understand how you can break down the problem into smaller pieces that
you can solve recursively. It is the same approach that is called divide and conquer in
algorithm design theory. Reducing a problem to smaller problems is the hard part. There
are two things to be careful about: Are the smaller problems really smaller? How do you
combine solutions from the smaller problems to solve the larger problem?

In the linear search we have worked on so far, we know that the recursive call
is looking at a smaller problem because each call is looking at a shorter sequence. It
doesn’t matter if we implement the function using lists or use an index into a vector. We
know that when we call recursively, we are looking at a shorter sequence. For functions
working on sequences, this is generally the case; and if you know that each time you
call recursively you are moving closer to a base case, you know that the function will
eventually finish its computation.

The recursion doesn’t always have to be on everything except the first element in a
sequence. For a binary search, for example, we can search in logarithmic time in a sorted
sequence by reducing the problem to half the size in each recursive call. The algorithm
works like this: if you have an empty sequence where you can’t find the element you are
searching for, you return FALSE. If the sequence is not empty, you check if the middle
element is the element you are searching for, in which case you return TRUE. If it isn’t, check
if it is smaller than the element you are looking for, in which case you call recursively on the
last half of the sequence, and if not, you call recursively on the first half of the sequence.

This sounds simple enough but first attempts at implementing this often end
up calling recursively on the same sequence again and again, never getting closer to
finishing. This happens if we are not careful when we pick the first or last half.
This implementation will not work. If you search for 0 or 5, you will get an infinite
recursion:

```{r}
binary_search <- function(element, x,
first = 1, last = length(x)) {
  if (last < first) return(FALSE) # empty sequence
middle <- (last - first) %/% 2 + first
if (element == x[middle]) {
TRUE
} else if (element < x[middle]) {
binary_search(element, x, first, middle)
} else {
binary_search(element, x, middle, last)
}
}
```
This is because you get a middle index that equals first, so you call recursively on
the same problem you were trying to solve, not a simpler one.
You can solve it by never including middle in the range you try to solve recursively;
after all, you only call the recursion if you know that middle is not the element you are
searching for:

```{r}
binary_search <- function(element, x,
first = 1, last = length(x)) {
if (last < first) return(FALSE) # empty sequence
middle <- (last - first) %/% 2 + first
if (element == x[middle]) {
TRUE
} else if (element < x[middle]) {
binary_search(element, x, first, middle - 1)
} else {
binary_search(element, x, middle + 1, last)
}
}
```

It is crucial that you make sure that all recursive calls actually are working on
a smaller problem. For sequences, that typically means making sure that you call
recursively on shorter sequences.

For trees, a data structure that is fundamentally recursive—a tree is either a leaf or
an inner node containing a number of children that are themselves also trees—we call
recursively on subtrees, thus making sure that we are looking at smaller problems in each
recursive call.

The functions we have written so far do not combine the results of the subproblems
we solve recursively. The functions are all search functions, and the result they return
is either directly found or the result the recursive functions return. It is not always that
simple, and often you need to do something with the result from the recursive call(s) to
solve the larger problem.

A simple example is computing the factorial. The factorial of a number n, n!, is equal
to with a basis case. It is very simple to write a recursive function to return the factorial,
but we cannot just return the result of a recursive call. We need to multiply the result we
get from the recursive call with n:
```{r}
factorial <- function(n) {
if (n == 1) 1
else n * factorial(n - 1)
}
```

Here I am assuming that is an integer and non-negative. If it is not, the recursion
doesn’t move us closer to the basis case and we will (in principle) keep going forever.
So here is another case where we need to be careful to make sure that when we call
recursively, we are actually making progress on solving the problem. This function is only
guaranteed for positive integers.

In most algorithms, we will need to do something to the results of recursive calls to
complete the function we are writing. As another example, besides factorial, we can
consider a function for removing duplicates in a sequence. Duplicates are elements that are
equal to the next element in the sequence. It is similar to the unique function built into R
except that this function only removes repeated elements that are right next to each other.

To write it, we follow the recipe for writing recursive functions. What is the base case?
An empty sequence doesn’t have duplicates, so the result is just an empty sequence. The
same is the case for a sequence with only one element. Such a sequence does not have
duplicated elements, so the result is just the same sequence. If we always know that the
input to our function has at least one element, we don’t have to worry about the first base
case, but if the function might be called on empty sequences, we need to take care of
both. For sequences with more than one element, we need to check if the first element
equals the next. If it does, we should just return the rest of the sequence, thus removing
a duplicated element. If it does not, we should return the first element together with the
rest of the sequence where duplicates have been removed.

To get a solution to the general problem, we have to combine the smaller solution
we get from the recursive call with information in the larger problem. If the first element
is equal to the first element we get back from the recursive call, we have a duplicate and
should just return the result of the recursive call. If not, we need to combine the first
element with the next list from the recursive call.

We can also implement this function for vectors. To avoid copying vectors each time
we remove a duplicate, we can split that function into two parts. First, we find the indices
of all duplicates and then we remove these from the vector in a single operation:

```{r}
find_duplicates <- function(x, i = 1) {
if (i >= length(x)) return(c())
rest <- find_duplicates(x, i + 1)
if (x[i] == x[i + 1]) c(i, rest)
else rest
}
vector_rm_duplicates <- function(x) {
dup <- find_duplicates(x)
x[-dup]
}
vector_rm_duplicates(c(1, 1, 2, 2))
```

R already has a built-in function for finding duplicates, called duplicated, and
we could implement find_duplicates using it (it returns a boolean vector, but we can
use the function which to get the indices of the TRUE values). It is a good exercise to
implement it ourselves, though.

The structure is very similar to the list version, but here we return the result of the
recursive call together with the current index if it is a duplicate and just the result of the
recursive call otherwise.

This solution isn’t perfect. Each time we create an index vector by combining it with
the recursive result, we are making a copy, so the running time will be quadratic in the
length of the result (but linear in the length of the input). We can turn it into a linear time
algorithm in the output as well by making a next list instead of a vector of the indices, and
then translate that into a vector in the remove duplicates function before we index into
the vector x to remove the duplicates. You can experiment with that as an exercise.

#### Tail-Recursion

Functions, such as our search functions, that return the result of recursive call without
doing further computation on it are called tail-recursive. Such functions are particularly
desired in functional programming languages because they can be translated into loops,
removing the overhead involved in calling functions. R, however, does not implement this
tail-recursion optimization. There are good but technical reasons why, having to do with
scopes. This doesn’t mean that we cannot exploit tail-recursion and the optimizations
possible if we write our functions to be tail-recursive. We just have to translate our
functions into loops explicitly. We cover that in the next section. First, let’s look at a
technique for translating an otherwise not tail-recursive function into one that is.
As long as you have a function that only calls recursively zero or one time, it is a very
simple trick. You pass along values in recursive calls that can be used to compute the final
value once the recursion gets to a base case.

As a simple example, we can take the factorial function. The way we wrote it above
was not tail-recursive. We called recursively and then multiplied to the result:

```{r}
factorial <- function(n) {
if (n == 1) 1
else n * factorial(n - 1)
}
```
We can translate this into a tail-recursive function by passing the product of the
numbers we have seen so far along to the recursive call. Such a value that is passed along
is typically called an accumulator. The tail-recursive function would look like this:

```{r}
factorial <- function(n, acc = 1) {
if (n == 1) acc
else factorial(n - 1, acc * n)
}
```

Similarly, we can take the find_duplicates function we wrote and turn it into a tailrecursive
function. The original function looks like this:

```{r}
find_duplicates <- function(x, i = 1) {
if (i >= length(x)) return(c())
rest <- find_duplicates(x, i + 1)
if (x[i] == x[i + 1]) c(i, rest) else rest
}
```

It needs to return a list of indices, so that is what we pass along as the accumulator:

```{r}
find_duplicates <- function(x, i = 1, acc = c()) {
if (i >= length(x)) return(acc)
if (x[i] == x[i + 1]) find_duplicates(x, i + 1, c(acc, i))
else find_duplicates(x, i + 1, acc)
}
```

All functions that call themselves recursively at most once can equally easily be
translated into tail-recursive functions using an appropriate accumulator.

#### Runtime Considerations
Now for the bad news. All the techniques I have shown you in this chapter for writing
pure functional programs using recursion instead of loops are not actually the best way to
write programs in R.

You will want to write pure functions, but relying on recursion instead of loops
comes at a runtime cost. We can replace the recursive linear search function with one that
uses a for loop to see how much overhead we incur.

The recursive function looked like this:

```{r}
r_lin_search <- function(element, sequence, i = 1) {
if (i > length(sequence)) FALSE
else if (sequence[i] == element) TRUE
else r_lin_search(element, sequence, i + 1)
}
```

A version using a for loop could look like this:

```{r}
l_lin_search <- function(element, sequence) {
for (e in sequence) {
if (e == element) return(TRUE)
}
return(FALSE)
}
```

We can use the function microbenchmark from the microbenchmark package to
compare the two. If we search for an element that is not contained in the sequence we
search in, we will have to search through the entire sequence, so we can use that worstcase
scenario for the performance measure:

```{r}
library(microbenchmark)
x <- 1:1000
microbenchmark(r_lin_search(-1, x),
l_lin_search(-1, x))
```

The recursive function is almost an order of magnitude slower than the function that
uses a loop. Keep that in mind if people tell you that loops are slow in R; they might be,
but recursive functions are slower.

It gets worse than that. R has a limit as to how deep you can call a function
recursively, and if we were searching in a sequence longer than about a thousand
elements, we would reach this limit and R would terminate the call with an error.

This doesn’t mean that reading this chapter was a complete waste of time. It can be
very useful to think in terms of recursion when you are constructing a function. There is a
reason why divide and conquer is frequently used to solve algorithmic problems. You just
want to translate the recursive solution into a loop once you have designed the function.

For functions such as linear search, we would never program a solution as a
recursive function in the first place. The for loop version is much easier to write and
much more efficient. Other problems are much easier to solve with a recursive algorithm,
and there the implementation is also easier by first thinking in terms of a recursive
function. The binary search is an example of such a problem. It is inherently recursive
since we solve the search by another search on a shorter string. It is also less likely to hit
the allowed recursion limit since it will only call recursively a number of times that is
logarithmic in the length of the input, but that is another issue.


### Filter, Map, and Reduce

We will just look at three
general methods that are used in functional programming instead of loops and instead
of explicitly writing recursive functions. They are really three different patterns for
computing on sequences, and they come in different flavors in different functions, but
just these three allow you do almost anything you would otherwise do with loops. Note
the emphasis on almost!

These three functions do not replace everything you can do with loops. You can
replace for loops, where you already know how much you are looping over, but they
cannot substitute for while and repeat loops. Still, by far the most loops you write in R
are for loops, and in general, you can use these functions to replace those.
The functions, or patterns, are Filter, Map, and Reduce. Filter takes a sequence and
a predicate, a function that returns a boolean value, and it returns a sequence where all
elements when the predicate was true are included and the rest are removed. Map evaluates
a function on each item in a sequence and returns a sequence with the results of evaluating
the function.

Reduce takes a
sequence and a function and evaluates the function repeatedly to reduce the sequence to a
single value. This pattern is also called fold in some programming languages.

#### The General Sequence Object in R Is a List

Sequences come in two flavors in R: vectors and lists. Vectors can only contain basic
types, and all elements in a vector must have the same type. Lists can contain a sequence
of any type, and the elements in a list can have different types. Lists are thus more general
than vectors and are often the building blocks of data structures such as the “next lists”
and the tree.

It, therefore, comes as no surprise that general functions for working on sequences
would work on lists. The three functions, Filter, Map, and Reduce, are also happy to take
vectors, but they are treated just as if you had explicitly converted them to lists first. The
Reduce function returns a value, not a sequence, of a type that depends on its input, while
Filter and Map both return sequences in the form of a list.

From a programming perspective, it is just as easy to work with lists as it is to work
with vectors, but some functions do expect vectors—plotting functions and functions
for manipulating data frames, for example. So sometimes you will have to translate a list
from Filter or Map into a vector. You can do this with the function unlist. This function
will convert a list into a vector when this is possible, that is when all elements are of the
same basic type, and otherwise will just give you the list back. We will use unlist in many
examples in this chapter just because it makes the output nicer to look at, but in most
programs, we would not bother doing so until we really need a vector. A list is just as good
for storing sequences.
For example, this code:

```{r}
list(1, 2, 3, 4)
```
gives us a much longer output listing to put in the book than:
```{r}
1:4
```
If you follow along in front of your compute you can see the results with and without
unlist to get a feeling for the differences.
You rarely need to convert sequences the other way, from vectors to lists. Functions
that work on lists usually also work on vectors, but if you want to, you could use the as.
list function and not the list function. The former gives you a list with one element per
element in the vector:
```{r}
as.list(1:4)
```
whereas the latter gives you a list with a single element that contains the vector:
```{r}
list(1:4)
```

#### Filtering Sequences
The Filter function is the simplest of the three main functions we cover in this chapter.
It simply selects a subset of a sequence based on a predicate. A predicate is a function
that returns a single boolean value, and Filter will return a list of elements where the
predicate returned TRUE and discard the elements where the predicate returned FALSE:

```{r}
is_even <- function(x) x %% 2 == 0
unlist(Filter(is_even, 1:10))
```
Here we have used a vector as input to Filter, but any list will do, and we do not
need to limit it to sequences of the same type:
```{r}
s <- list(a = 1:10, b = list(1,2,3,4,5,6),
          c = y ~ x1 + x2 + x3, d = vector("numeric"))
Filter(function(x) length(x) > 5, s)
```

When printed, the results aren’t pretty, but we can’t solve that with unlist in this
case. Using unlist we would get a vector, but not one remotely reflecting the structure of
the result. The vector a and list b would be flattened into a single vector.

#### Mapping Over Sequences

The Map function evaluates a function for each element in a list and returns a list with the
results:

```{r}
unlist(Map(is_even, 1:5))
```
can be applied on lists of different types:
```{r}
s <- list(a = 1:10, b = list(1,2,3,4,5,6),
c = y ~ x1 + x2 + x3, d = vector("numeric"))
unlist(Map(length, s))
```

Map can be applied to more than one sequences if the function you provide it takes a
number of parameters that matches the number of sequences:

```{r}
unlist(Map(`+`, 1:5, 1:5))
```

In this example, we use the function `+`, which takes two arguments, and we give
the Map function two sequences, so the result is the component-wise addition.
You can pass along named parameters to a Map call, either directly as a named
parameter:

```{r}
x <- 1:10
y <- c(NA, x)
s <- list(x = x, y = y)
unlist(Map(mean, s))
unlist(Map(mean, s, na.rm = TRUE))
```
or as a list provided to the MoreArgs parameter:
```{r}
unlist(Map(mean, s, MoreArgs = list(na.rm = TRUE)))
```
For a single value, the two approaches work the same, but the semantics are slightly
different, which comes into play when providing arguments that are sequences. Providing
a named argument directly to Map works just as if providing an unnamed argument
(except that you can pick a specific variable by name instead of by position). So Map
assumes that you want to apply your function to every element of the argument. The
reason this works with a single argument is that, as R generally does, the shorter sequence
is repeated as many times as needed. With a single argument, that is exactly what we
want, but it isn’t necessarily with a sequence.

If we want that behavior, we can just use the named argument to Map, but if we want
the function to be called with the entire sequence each time it is called, we must put the
sequence as an argument to the MoreArgs parameter.

As an example, we can consider our trusted friend the scale function and make a
version where a vector, x, is scaled by the mean and standard deviation of another vector, y:
```{r}
scale <- function(x, y) (x - mean(y))/sd(y)
```
If we just provide Map with two arguments for scale, it will evaluate all pairs
independently (and we will get a lot of NA values because we are calling the sd function on
a single value):

```{r}
unlist(Map(scale, 1:10, 1:5))
```

The same happens if we name parameter y:
```{r}
unlist(Map(scale, 1:10, y = 1:5))
```
But if we use MoreArgs, the entire vector y is provided to scale in each call:
```{r}
unlist(Map(scale, 1:10, MoreArgs = list(y = 1:5)))
```

Just as Filter, Map is not restricted to work on vectors, so we can map over arbitrary
types as long as our function can handle the different types:

```{r}
s <- list(a = 1:10, b = list(1,2,3,4,5,6),
c = y ~ x1 + x2 + x3, d = vector("numeric"))
unlist(Map(length, s))
```

#### Reducing Sequences
While Filter and Map produce lists, the Reduce function transforms a list into a value. Of
course that value can also be a list, since lists are also values, but Reduce doesn’t simply
process each element in its input list independently. Instead, it summarizes the list by
applying a function iteratively to pairs. You provide it a function, f, of two elements, and
it will first call f on the first two elements in the list. Then it will take the result of this and
call f with this and the next element, and continue doing that through the list.

So calling `Reduce(f, 1:5)` will be equivalent to calling:
```{r,eval=FALSE}
f(f(f(f(1, 2), 3), 4), 5)
```
It is just more readable to write `Reduce(f, 1:5)`.

We can see this in action using `+` as the function:
```{r}
Reduce(`+`, 1:5)
```
You can also get the step-wise results back by using the parameter accumulate. This
will return a list of all the calls to f and include the first value in the list, so Reduce(f, 1:5)
will return the list:

```{r,eval=FALSE}
c(1, f(1, 2), f(f(1 ,2), 3), f(f(f(1, 2), 3), 4),
f(f(f(f(1, 2), 3), 4), 5))
```

So for addition we would get:

```{r}
Reduce(`+`, 1:5, accumulate = TRUE)
```

By default Reduce does its computations from left to right, but by setting the option
right to TRUE, you instead get the results from right to left:

```{r}
Reduce(`+`, 1:5, right = TRUE, accumulate = TRUE)
```
For an associative operation like `+`, this will, of course, give the same result if we do
not ask for the accumulative function calls.

In many functional programming languages, which all have this function, although
it is sometimes called fold or accumulate, you need to provide an initial value for the
computation. This is then used in the first call to f, so the folding instead starts with
f(init, x[1]) if init refers to the initial value and x is the sequence.

You can also get that behavior in R by explicitly giving Reduce an initial value through
parameter init:
```{r}
Reduce(`+`, 1:5, init = 10, accumulate = TRUE)
```
You just don’t need to specify this initial value that often. In languages that require
it, it is used to get the right starting point when accumulating results. For addition, we
would use 0 as an initial value if we want Reduce to compute a sum because adding zero
to the first value in the sequence would just get us the first element. For multiplication,
we would instead have to use 1 as the initial value since that is how the first function
application will just give us the initial value.

In R we don’t need to provide these initial values if we are happy with just having the
first function call be on the first two elements in the list. So multiplication works just as
well as addition without providing init:

```{r}
Reduce(`*`, 1:5)
Reduce(`*`, 1:5, accumulate = TRUE)
Reduce(`*`, 1:5, right = TRUE, accumulate = TRUE)
```

You wouldn’t normally use Reduce for summarizing values as their sum or product.
There are already functions in R for this (sum and prod, respectively), and these are much
faster as they are low-level functions implemented in C while Reduce has to be a highlevel
function to handle arbitrary functions. For more complex data where we do not
already have a function to summarize a list, Reduce is often the way to go.


```{r}
samples <- replicate(3, sample(1:10, replace = TRUE),
simplify = FALSE)
str(samples)
Reduce(intersect, samples)
```
We have a list of three vectors each with ten samples of the numbers from 1 to 10,
and we want to get the intersection of these three lists. That means taking the intersection
of the first two and then taking the intersection of that result and the third list. Perfect for
Reduce. We just combine it with the intersection function.


### The Apply Family of Functions

The Map function is a general solution for mapping over elements in a list, but R has a
whole family of Map-like functions that operate on different types of input. These are
all named “something”-apply. The Map
function is actually just a wrapper around one of these, the function mapply, and since
we have already seen Map in use, I will not discuss mapply here, but I will give you a brief
overview of the other functions in the apply family.

#### apply
The apply function works on matrices and higher-dimensional arrays instead of
sequences. It takes three parameters, plus any additional parameters that should just
be passed along to the function called. The first parameter is the array to map over, the
second determines which dimension(s) should be marginalized, and the third gives the
function that should be applied.

We can see this in action by creating a matrix to apply over:
```{r}
(m <- matrix(1:6, nrow=2, byrow=TRUE))
```

To see what is actually happening, we will create a function that collects the data that
it gets so we can see exactly what it is called with:

```{r}
collaps_input <- function(x) paste(x, collapse = ":")
```
If we marginalize on rows, it will be called on each of the two rows and the function
will be called with the entire row vectors:

```{r}
apply(m, 1, collaps_input)
```

If we marginalize on columns, it will be called on each of the three columns and
produce tree strings:

```{r}
apply(m, 2, collaps_input)
```

If we marginalize on both rows and columns, it will be called on each single element
instead:
```{r}
apply(m, c(1, 2), collaps_input)
```

#### The tapply Function

The tapply function works on so-called ragged tables, tables where the rows can have
different lengths. You cannot directly make an array with different sizes of dimensions
in rows, but you can use a flat vector combined with factors that indicate which virtual
dimensions you are using. The tapply function groups the vectors according to a factor
and then calls its function with each group:

```{r}
(x <- rnorm(10))
(categories <- sample(c("A", "B", "C"), size = 10, replace = TRUE))
tapply(x, categories, mean)
```

You can use more than one factor if you wrap the factors in a list:

```{r}
(categories2 <- sample(c("X", "Y"), size = 10, replace = TRUE))
tapply(x, list(categories, categories2), mean)
```


### Functional Programming in purrr

The Filter, Map, and Reduce functions are the building blocks of many functional
algorithms, in addition to recursive functions. However, many common operations
require various combinations of the three functions and combinations with unlist, so
writing functions using only these three or four functions means building functions from
the most basic building blocks. This is not efficient, so you want to have a toolbox of more
specific functions for common operations.

The package purrr implements a number of such functions for more efficient
functional programming, in addition to its own versions of Filter, Map, and Reduce.
Complete coverage of the purrr package is beyond the scope of this book, but I will give
a quick overview of the functions available in the package and urge you to explore the
package more if you are serious in using functional programming in R.

```{r}
library(purrr)
triple <- function(x) x * 3
map(1:3, triple)
```

You might wonder why this function is called map(). What does it have to do with depicting physical features of land or sea map? In fact, the meaning comes from mathematics where map refers to “an operation that associates each element of a given set with one or more elements of a second set”. This makes sense here because map() defines a mapping from one vector to another. (“Map” also has the nice property of being short, which is useful for such a fundamental building block.)

The implementation of map() is quite simple. We allocate a list the same length as the input, and then fill in the list with a for loop. The heart of the implementation is only a handful of lines of code:

```{r}
simple_map <- function(x, f, ...) {
  out <- vector("list", length(x))
  for (i in seq_along(x)) {
    out[[i]] <- f(x[[i]], ...)
  }
  out
}
```

The real purrr::map() function has a few differences: it is written in C to eke out every last iota of performance.
The base equivalent to map() is lapply(). The only difference is that lapply() does not support the helpers that you’ll learn about below, so if you’re only using map() from purrr, you can skip the additional dependency and use lapply() directly.

####  Producing atomic vectors

`map()` returns a list, which makes it the most general of the map family because you can put anything in a list. But it is inconvenient to return a list when a simpler data structure would do, so there are four more specific variants: `map_lgl()`, `map_int()`, `map_dbl()`, and `map_chr()`. Each returns an atomic vector of the specified type:

```{r}
# map_chr() always returns a character vector
map_chr(mtcars, typeof)
# map_lgl() always returns a logical vector
map_lgl(mtcars, is.double)
# map_int() always returns a integer vector
n_unique <- function(x) length(unique(x))
map_int(mtcars, n_unique)
# map_dbl() always returns a double vector
map_dbl(mtcars, mean)
```

purrr uses the convention that suffixes, like _dbl(), refer to the output. All map_*() functions can take any type of vector as input. These examples rely on two facts: mtcars is a data frame, and data frames are lists containing vectors of the same length. 

All map functions always return an output vector the same length as the input, which implies that each call to .f must return a single value. If it does not, you’ll get an error:

```{r,eval=FALSE}
pair <- function(x) c(x, x)
map_dbl(1:2, pair)
#> Error: Result 1 must be a single double, not an integer vector of length 2
```

This is similar to the error you’ll get if .f returns the wrong type of result:

```{r,eval=FALSE}
map_dbl(1:2, as.character)
#> Error: Can't coerce element 1 from a character to a double
```



keep() and discard() … keep and discard elements of a list or vector based on a predicate function. A predicate function is a function that returns TRUE or FALSE. So is.factor() is a predicate function, because it always returns TRUE or FALSE, while round() is not.

For example, we can keep() all elements of our list that are less than 3 with the following code

```{r}
my_vector <- c(1.0212, 2.483, 3.189, 4.5938)
keep(my_vector, ~ .x < 3)
discard(my_vector, ~ .x < 3)
```

#### map_if()

What if you’re not sure about the types of every element in your list, and you want to apply a function that needs the input to be of a certain type? For example, let’s say we wanted to add 10 to every element of a list.
```{r}
mixed_list <- list("happy", 2L, 4.39)

add_ten <- function(n) {
  n + 10
}

```
```{r,eval=F}
map(mixed_list, add_ten)
##  Error in n + 10 : non-numeric argument to binary operator
```

We get an error since we’re trying to add 10 to “happy”, which isn’t numeric!

This is where map_if() comes in handy.  Here, we know that condition is that the element needs to be numeric. Let’s try again with map_if():
```{r}
map_if(mixed_list, is.numeric, add_ten)
```
Alright! We see it skipped over “happy”, preserving it as is, and added ten to the two numeric elements.

#### every() and some()

Sometimes you have a giant list and want to know whether each element meets a condition, like being numeric. You can use `every()`, which will check if every element of a list satisfies a predicate function:

```{r}
every(mixed_list, is.numeric)
```

Since “happy” is the first element of mixed_list, it doesn’t past the test.
If we want to be less strict and just check if some of the elements satisfies a predicate function, we can use `some()` instead:
```{r}
some(mixed_list, is.numeric)
```

In this case, since some elements of `mixed_list` were numeric, we got `TRUE`.

#### Modifying functions
`purrr` includes adverbs - functions that take a function and return a modified version (just as an adverb modifies a verb). Let’s check out a few!

#### negate()
negate() … negates a predicate function (aren’t the purrr function names great?). For example, let’s say you want to check which elements of a list were not null. This is how you would do it with map_lgl (which returns a logical vector rather than a list):
```{r}
lst <- list("a", 3, 22, NULL, "q", NULL)
map_lgl(lst, ~ !is.null(.))
```

This works, but it’s not super easy to read. Instead, we can make an `is_not_null()` function using `negate()`:
```{r}
is_not_null <- negate(is.null)
map_lgl(lst, is_not_null)
```

#### partial()
You probably have a couple functions where you almost always use an extra argument, like mean() with na.rm = TRUE or round() with digits = 1. You can use partial() to create a new function where those are always specified, saving you some repetitive typing!
```{r}
mean(c(10, NA, 5, 7))
mean(c(10, NA, 5, 7), na.rm = TRUE)
my_mean <- partial(mean, na.rm = TRUE)
my_mean(c(10, NA, 5, 7))
my_round <- partial(round, digits = 1)
my_round(10.484)
```

#### safely() and possibly()
We saw earlier that map_if() could be used where we have a condition we want to be met before applying a function. In our case, we used it to avoid an error, but you could also use it to meet a condition like the number being negative or greater than a threshold. But what if you don’t know where errors could come from but want to handle them? This is where safely() and possibly() come in.


If you’re not interested in what the error is, you should use possibly(). In addition to the function it’s wrapping around, you need to specify the argument otherwise, which is what you want to return if there is an error. Let’s take a look:
```{r}
possibly_add_ten <- possibly(add_ten, otherwise = "I'm not numeric!")
map(mixed_list, possibly_add_ten)
```

Side-note - I find the the double-brackets confusing. We’ve got them since our list isn’t named. Let’s set the name of each element with purrr's set_names(), giving our elements the very creative names a, b, and c.
```{r}
mixed_list <- set_names(mixed_list, c("a", "b", "c"))
mixed_list
```
Well … mildly easier to read at least.

On the other hand, sometimes you do want to know what the error is. If that’s the case, you can use safely() instead:
```{r}
map(mixed_list, safely(add_ten))
```

safely() returns a list of lists. Each element from the original list has two entries: result and error. One is always NULL - if there was an error, result is NULL and error is the error message; if there wasn’t an error, the result is the result and error is NULL. If you want to get back all the errors or results, you can use another handy feature of map(). If you give map() a string as the second argument, for each element of the list, it will return the element inside of it with that name.

```{r}
safely_add_ten <- safely(add_ten)
mixed_list %>%
  map(safely_add_ten) %>%
  map("error")
```
In the previous example, we probably wouldn’t be interested in the instances where errors were NULL. We could use discard(is.null), but purrr actually provides a function just for the purpose of getting rid of NULLs: compact().
```{r}
mixed_list %>%
  map(safely_add_ten) %>%
  map("error") %>%
  compact()
```


compose() lets you string together multiple functions. Let’s say you want to add_ten(), take the log, and then round a vector of numbers. You could do either of these:
```{r}
c(1, 20, 500) %>%
  add_ten() %>%
  log() %>%
  round()
round(log(add_ten(c(1, 20, 500))))
```

But you could also make a new function using compose(). You give compose() functions to execute in order from right to left (just like we have written above):

```{r}
add_ten_log_and_round <- compose(round, log, add_ten)
add_ten_log_and_round(c(1, 20, 500))
```

compose() is great for simplifying your code if you’re going to use a sequence of functions again and again.

What if we wanted to round to the nearest tenth instead? You can combine compose() with partial()!
```{r}
round_tenth <- partial(round, digits = 1)
add_ten_log_and_round_tenth <- compose(round_tenth, log, add_ten)
add_ten_log_and_round_tenth(c(1, 20, 500))
```



The following example uses purrr to solve a fairly realistic problem: split a data frame into pieces, fit a model to each piece, compute the summary, then extract the R-square.

```{r}
library(purrr)
mtcars %>%
  split(.$cyl) %>% # from base R
  map(~ lm(mpg ~ wt, data = .)) %>%
  map(summary) %>%
  map_dbl("r.squared")
```

This example illustrates some of the advantages of purrr functions over the equivalents in base R:

- The first argument is always the data, so purrr works naturally with the pipe.

- All purrr functions are type-stable. They always return the advertised output type (map() returns lists; map_dbl() returns double vectors), or they throw an error.

- All `map()` functions either accept function, formulas (used for succinctly generating anonymous functions), a character vector (used to extract components by name), or a numeric vector (used to extract by position).






